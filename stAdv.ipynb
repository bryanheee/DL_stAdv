{"cells":[{"cell_type":"markdown","metadata":{"id":"SWo9IZOaf7lq"},"source":["All imports\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_t-6Fl3Cf7ls","outputId":"17089abb-034d-48bc-beb7-aae7d7f50e5b"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ehaan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Ehaan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n","  warn(\n"]}],"source":["import numpy as np\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import math\n","import random\n","import time\n","from torchsummary import summary"]},{"cell_type":"markdown","metadata":{"id":"09riOLktf7lt"},"source":["Data download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDDK3TOrf7lu"},"outputs":[],"source":["# Define transformations to apply to the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Convert images to tensors\n","    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the pixel values to be in range [-1, 1]\n","])\n","\n","# Download and load the training set\n","trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","# Download and load the test set\n","testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","testloader = DataLoader(testset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPHaR36df7lv"},"outputs":[],"source":["def get_images_labels(dataset):\n","    '''\n","    Function for getting an instance of the image and label for all 10 different digits\n","    'dataset' can be the test or trainset\n","    '''\n","    digits_xy = {digit: None for digit in range(10)}\n","    # dataset\n","    for i in range(10):\n","        indices = (dataset.targets == i).nonzero().view(-1)\n","        rand_index = random.randrange(indices.size(dim=0))\n","        digits_xy[i] = dataset[indices[rand_index]][0].squeeze()\n","    return digits_xy\n","\n","\n","def show_images(digits):\n","    # a figure with 2 rows and 5 columns\n","    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n","\n","    axs = axs.flatten()\n","\n","    for index, (digit, image) in enumerate(digits.items()):\n","        axs[index].imshow(image.squeeze().detach().numpy(), cmap='gray')\n","        axs[index].set_title(f'Digit: {digit}')\n","        axs[index].axis('off')\n","\n","    # adjust layout to prevent overlap\n","    plt.tight_layout()\n","    plt.show()\n","\n","def get_adv_targets(digits):\n","    return {k : (v, [i for i in range(10) if i!=k]) for k, v in digits.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfEs5_NVf7lv"},"outputs":[],"source":["def get_adv_example_3(image, flow_field):\n","    '''\n","    'image' is the unperturbed image tensor with shape [H=28, W=28]\n","    'flow_field' is the flowfield used to obtain x_adv from x, f has shape [H=28, W=28, 2]\n","    returns x_adv with shape [H=28, W=28]\n","    '''\n","    H, W = image.size(0), image.size(1)\n","\n","    # Generate grid of indices for interpolation\n","    grid_h, grid_w = torch.meshgrid(torch.arange(H), torch.arange(W))\n","    grid_h = grid_h.float()\n","    grid_w = grid_w.float()\n","\n","    # Add flow field to grid indices\n","    u = grid_h + flow_field[..., 0]\n","    v = grid_w + flow_field[..., 1]\n","\n","    # Ensure indices are within bounds\n","    u = torch.clamp(u, 0, H - 1)\n","    v = torch.clamp(v, 0, W - 1)\n","\n","    # Indices for top-left, top-right, bottom-left, bottom-right pixels\n","    h0 = u.floor().long()\n","    w0 = v.floor().long()\n","    h1 = h0 + 1\n","    w1 = w0 + 1\n","\n","    # Bilinear interpolation weights\n","    fu = u - h0.float()\n","    fv = v - w0.float()\n","\n","    # Ensure indices are within bounds\n","    h0 = torch.clamp(h0, 0, H - 1)\n","    h1 = torch.clamp(h1, 0, H - 1)\n","    w0 = torch.clamp(w0, 0, W - 1)\n","    w1 = torch.clamp(w1, 0, W - 1)\n","\n","    # Interpolate\n","    top_left = image[h0, w0]\n","    top_right = image[h0, w1]\n","    bottom_left = image[h1, w0]\n","    bottom_right = image[h1, w1]\n","\n","    top_interp = (1 - fu) * top_left + fu * top_right\n","    bottom_interp = (1 - fu) * bottom_left + fu * bottom_right\n","\n","    interpolated_value = (1 - fv) * top_interp + fv * bottom_interp\n","\n","    return interpolated_value\n","\n","def L_flow_2(flow_field):\n","    delta_u = flow_field[:, :, 0]\n","    delta_v = flow_field[:, :, 1]\n","\n","    # Pad the deltas to handle the borders (assumes zero padding)\n","    delta_u_padded = F.pad(delta_u, pad=(1, 1, 1, 1), mode='constant', value=0)\n","    delta_v_padded = F.pad(delta_v, pad=(1, 1, 1, 1), mode='constant', value=0)\n","\n","    # Calculate differences between adjacent pixels (up, down, left, right)\n","    diff_u_up = delta_u_padded[2:, 1:-1] - delta_u_padded[1:-1, 1:-1]\n","    diff_u_down = delta_u_padded[:-2, 1:-1] - delta_u_padded[1:-1, 1:-1]\n","    diff_v_left = delta_v_padded[1:-1, 2:] - delta_v_padded[1:-1, 1:-1]\n","    diff_v_right = delta_v_padded[1:-1, :-2] - delta_v_padded[1:-1, 1:-1]\n","\n","    # Compute L2 norms for each direction and sum them up\n","    L_flow = torch.sum(torch.sqrt(diff_u_up**2 + diff_v_left**2 + 1e-8)) + torch.sum(torch.sqrt(diff_u_down**2 + diff_v_right**2 + 1e-8))\n","    # print(f'L_flow output has the following shape = {L_flow.size()}')\n","    return L_flow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAJowNnGf7lw"},"outputs":[],"source":["def closure_factory(x, flow_field, model, t, tau, kappa, optimizer):\n","    \"\"\"\n","    A factory function that creates and returns a closure function with the right image x,\n","    an inititial flow_field to optimize, a model and target tfor optimization.\n","\n","    Parameters:\n","    - x: the unperturbed image tensor.\n","    - flow_field: the flow field tensor with requires_grad=True.\n","    - model: the trained neural network model to use for generating adversarial examples.\n","    - t: the target label, different from the true label of x.\n","\n","    Returns:\n","    A closure function suitable for the optimizer.\n","    \"\"\"\n","    def closure():\n","        # print(\"...\")\n","        optimizer.zero_grad()\n","        flow_field.requires_grad_(True)\n","        #adversarial_example = flow_st(x.unsqueeze(0).unsqueeze(0),flows).squeeze()\n","        adversarial_example = get_adv_example_3(x, flow_field)\n","        output = model(adversarial_example.unsqueeze(0).unsqueeze(0))\n","        # Masking out the value at [0, t]\n","        masked_output = output.clone()\n","        masked_output[0, t] = float('-inf')  # Set it to negative infinity or any value that ensures it won't be selected as the maximum\n","        # Find the maximum value in the masked output\n","        max_value_excluding_t = torch.max(masked_output)\n","        # Set the t'th element of the difference tensor to be the original value of output[0, t]\n","        output_diff = max_value_excluding_t-output[0, t]\n","        L_adv = torch.max(output_diff, kappa).values\n","        L_flow = L_flow_2(flow_field)\n","        loss = L_adv + tau * L_flow\n","        loss.backward()\n","        return loss.item()\n","\n","    return closure\n","def get_optimized_x_adv(image_x, model, target, num_steps, tau, kappa,\n","                        lr, max_iter, max_eval,\n","                        tolerance_grad, tolerance_change,\n","                        history_size, line_search_fn):\n","    size = image_x.squeeze().size()\n","    delta = 1e-15\n","\n","    flow_field = torch.zeros(*size, 2) + delta\n","    flow_field.requires_grad_(True)\n","\n","    optimizer = torch.optim.LBFGS([flow_field], lr=lr, max_iter=max_iter, max_eval=max_eval,\n","                              tolerance_grad=tolerance_grad, tolerance_change=tolerance_change,\n","                              history_size=history_size,\n","                              line_search_fn=line_search_fn)\n","\n","    closure = closure_factory(image_x, flow_field, model, target, tau, kappa, optimizer)\n","    for step in range(num_steps):\n","        try:\n","          optimizer.step(closure)\n","        except ValueError as e:\n","          error_message = str(e)\n","          print(f\"Caught an error: {error_message}\")\n","    return get_adv_example_3(image_x, flow_field)\n","\n","\n","def show_x_and_x_adv(x, x_adv, t):\n","    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n","    axs[0].imshow(x_adv.squeeze().detach().numpy(), cmap='gray')\n","    axs[0].set_title(f'Adversarial Example with target t = {t}')\n","    axs[0].axis('off')\n","    axs[1].imshow(x, cmap='gray')\n","    axs[1].set_title('Original Image')\n","    axs[1].axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV2EmTkTf7lx"},"outputs":[],"source":["testset_with_random_targets = torch.load('testset_with_random_targets.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXdVhPkvf7lx","outputId":"21c38daf-d9e1-4db0-c9f1-464ebcabbab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 64, 24, 24]          1,600\n","├─Conv2d: 1-2                            [-1, 64, 20, 20]          102,400\n","├─Dropout: 1-3                           [-1, 64, 20, 20]          --\n","├─Linear: 1-4                            [-1, 128]                 3,276,928\n","├─Dropout: 1-5                           [-1, 128]                 --\n","├─Linear: 1-6                            [-1, 10]                  1,290\n","==========================================================================================\n","Total params: 3,382,218\n","Trainable params: 3,382,218\n","Non-trainable params: 0\n","Total mult-adds (M): 45.16\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.48\n","Params size (MB): 12.90\n","Estimated Total Size (MB): 13.38\n","==========================================================================================\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 64, 21, 21]          4,160\n","├─ReLU: 1-2                              [-1, 64, 21, 21]          --\n","├─Dropout: 1-3                           [-1, 64, 21, 21]          --\n","├─Conv2d: 1-4                            [-1, 128, 16, 16]         295,040\n","├─ReLU: 1-5                              [-1, 128, 16, 16]         --\n","├─Conv2d: 1-6                            [-1, 128, 12, 12]         409,728\n","├─ReLU: 1-7                              [-1, 128, 12, 12]         --\n","├─Dropout: 1-8                           [-1, 128, 12, 12]         --\n","├─Flatten: 1-9                           [-1, 18432]               --\n","├─Linear: 1-10                           [-1, 10]                  184,330\n","├─Softmax: 1-11                          [-1, 10]                  --\n","==========================================================================================\n","Total params: 893,258\n","Trainable params: 893,258\n","Non-trainable params: 0\n","Total mult-adds (M): 136.47\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.61\n","Params size (MB): 3.41\n","Estimated Total Size (MB): 4.02\n","==========================================================================================\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 128, 26, 26]         1,280\n","├─Conv2d: 1-2                            [-1, 64, 24, 24]          73,792\n","├─Dropout: 1-3                           [-1, 64, 24, 24]          --\n","├─Linear: 1-4                            [-1, 128]                 4,718,720\n","├─Dropout: 1-5                           [-1, 128]                 --\n","├─Linear: 1-6                            [-1, 10]                  1,290\n","==========================================================================================\n","Total params: 4,795,082\n","Trainable params: 4,795,082\n","Non-trainable params: 0\n","Total mult-adds (M): 47.97\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.94\n","Params size (MB): 18.29\n","Estimated Total Size (MB): 19.24\n","==========================================================================================\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 32, 26, 26]          320\n","├─Conv2d: 1-2                            [-1, 32, 24, 24]          9,248\n","├─Conv2d: 1-3                            [-1, 64, 10, 10]          18,496\n","├─Conv2d: 1-4                            [-1, 64, 8, 8]            36,928\n","├─Linear: 1-5                            [-1, 200]                 205,000\n","├─Dropout2d: 1-6                         [-1, 200]                 --\n","├─Linear: 1-7                            [-1, 200]                 40,200\n","├─Linear: 1-8                            [-1, 10]                  2,010\n","==========================================================================================\n","Total params: 312,202\n","Trainable params: 312,202\n","Non-trainable params: 0\n","Total mult-adds (M): 9.95\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.39\n","Params size (MB): 1.19\n","Estimated Total Size (MB): 1.58\n","==========================================================================================\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ehaan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","├─Conv2d: 1-1                            [-1, 32, 26, 26]          320\n","├─Conv2d: 1-2                            [-1, 32, 24, 24]          9,248\n","├─Conv2d: 1-3                            [-1, 64, 10, 10]          18,496\n","├─Conv2d: 1-4                            [-1, 64, 8, 8]            36,928\n","├─Linear: 1-5                            [-1, 200]                 205,000\n","├─Dropout2d: 1-6                         [-1, 200]                 --\n","├─Linear: 1-7                            [-1, 200]                 40,200\n","├─Linear: 1-8                            [-1, 10]                  2,010\n","==========================================================================================\n","Total params: 312,202\n","Trainable params: 312,202\n","Non-trainable params: 0\n","Total mult-adds (M): 9.95\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.39\n","Params size (MB): 1.19\n","Estimated Total Size (MB): 1.58\n","=========================================================================================="]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["Kernel_size = 5\n","N_channels = 64\n","P_1 = 0.25\n","P_2 = 0.5\n","in_features = 64 * 20 * 20\n","hidden_size =  128\n","output_size = 10\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Define the layers of the network\n","        self.Conv1 = nn.Conv2d(1, N_channels, Kernel_size, bias=False)  # Convolutional layer with 1 input channel, 32 output channels, and 3x3 kernel\n","        self.Conv2 = nn.Conv2d(N_channels, N_channels, Kernel_size, bias=False) # Convolutional layer with 32 input channels, 64 output channels, and 3x3 kernel\n","        self.Dropout1 = nn.Dropout(p=P_1)\n","        self.FC1 = nn.Linear(in_features, hidden_size)  # Fully connected layer with 64*6*6 input features and 128 output features\n","        self.Dropout2 = nn.Dropout(p=P_2)\n","        self.FC2 = nn.Linear(hidden_size, output_size)      # Fully connected layer with 128 input features and 10 output features (for 10 classes)\n","\n","    def forward(self, x):\n","        # A good programmer always debugs using print statements\n","        # Define the forward pass through the network\n","        x = F.relu(self.Conv1(x))   # Apply convolution, then ReLU activation\n","        #print(\"shape after first conv\", x.shape)\n","        x = F.relu(self.Conv2(x))   # Apply convolution, then ReLU activation\n","        #print(\"shape after second conv\",x.shape)\n","        x = self.Dropout1(x)    #Apply dropout\n","        #print(\"shape after first dropout\",x.shape)\n","        x = torch.flatten(x, start_dim=1) # Reshape the tensor for the fully connected layer\n","        #print(\"shape after flattening\",x.shape)\n","        x = F.relu(self.FC1(x))     # Apply ReLU activation to the first fully connected layer\n","        #print(\"shape after first FC\",x.shape)\n","        x = self.Dropout2(x)    #Apply dropout\n","        #print(\"shape after second dropout\",x.shape)\n","        x = self.FC2(x)      # Apply the second fully connected layer, then Softmax\n","        # Apply softmax activation function\n","        x = F.softmax(x, dim=1)\n","        #print(\"shape after second FC\",x.shape)\n","        return x\n","\n","modelA = Net()\n","modelA.load_state_dict(torch.load('model_A.pth'))\n","summary(modelA,(1,28,28))\n","\n","#Load model B\n","modelB = nn.Sequential(\n","    nn.Conv2d(1,64,8),\n","    nn.ReLU(),\n","    nn.Dropout(0.2),\n","    nn.Conv2d(64,128,6),\n","    nn.ReLU(),\n","    nn.Conv2d(128,128,5),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Flatten(start_dim=1),\n","    nn.Linear(18432,10),\n","    nn.Softmax(dim=1)\n",")\n","\n","modelB.load_state_dict(torch.load('model_B.pth'))\n","summary(modelB,(1,28,28))\n","\n","# Model C [table 4] trained on MNIST dataset\n","class ModelC(nn.Module):\n","\n","    # Specify layers\n","    def __init__(self):\n","        super(ModelC, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 128, 3) # mnist has single channel input\n","        self.conv2 = nn.Conv2d(128, 64, 3)\n","        self.drop1 = nn.Dropout(0.25)\n","        self.fc1   = nn.Linear(64*24*24, 128)\n","        self.drop2 = nn.Dropout(0.50)\n","        self.fc2   = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x)) # Conv(128, 3, 3) + Relu\n","        x = F.relu(self.conv2(x)) # Conv(64, 3, 3) + Relu\n","        x = self.drop1(x)         # Dropout(0.25)\n","        x = torch.flatten(x, 1)   # Flatten\n","        x = F.relu(self.fc1(x))   # FC(128) + Relu\n","        x = self.drop2(x)         # Dropout(0.50)\n","        x = F.relu(self.fc2(x))   # FC(10)\n","        return F.log_softmax(x, dim=1) # + Softmax\n","modelC = ModelC()\n","modelC.load_state_dict(torch.load('model_C.pth'))\n","summary(modelC,(1,28,28))\n","\n","class NetZ(nn.Module):\n","  def __init__(self):\n","    super(NetZ, self).__init__()\n","    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","    self.conv2 = nn.Conv2d(32, 32, 3, 1)\n","    self.conv3= nn.Conv2d(32, 64, 3, 1)\n","    self.conv4= nn.Conv2d(64, 64, 3, 1)\n","    self.dropout1 = nn.Dropout2d(0.5)\n","    self.fc1 = nn.Linear(1024, 200)\n","    self.fc2 = nn.Linear(200, 200)\n","    self.fc3 = nn.Linear(200, 10)\n","\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = F.relu(x)\n","    x = self.conv2(x)\n","    x = F.relu(x)\n","    x = F.max_pool2d(x, 2)\n","    x = self.conv3(x)\n","    x = F.relu(x)\n","    x = self.conv4(x)\n","    x = F.relu(x)\n","    x = F.max_pool2d(x, 2)\n","    x = torch.flatten(x,1)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.dropout1(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    return x\n","\n","modelZ = NetZ()\n","modelZ.load_state_dict(torch.load('model_Z.pth', map_location=torch.device('cpu')))\n","summary(modelZ,(1,28,28))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VD-rJ5fvf7lz"},"outputs":[],"source":["def generate_adv_test_get(testset_with_random_targets, model, start, end_exclusive, model_letter):\n","    \"\"\"\n","    - using default values for num_steps in get_optimized_x_adv function which is 5\n","    \"\"\"\n","    # To generate different versions of the adversarial dataset, we can change the hyperparameters in the get_optimized_x_adv function called below\n","    print(f'=============================== FINDING ADVERSARIAL EXAMPLES FOR MODEL {model_letter} =======================')\n","    ret = []\n","    counter = start\n","    for sample, label, target in testset_with_random_targets[start:end_exclusive]:\n","        # print(f'now finding x_adv for {counter} / {end_exclusive}')\n","        adv_example = None\n","        while adv_example is None:\n","            adv_example = get_optimized_x_adv(sample, model, target,\n","                                              num_steps= 50, tau=0.05, kappa=0, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-15,\n","                                              tolerance_change=1e-15, history_size=100, line_search_fn=\"strong_wolfe\")\n","        print(f'found x_adv for {counter} / {end_exclusive}')\n","        output = model(sample.unsqueeze(0).unsqueeze(0)).topk(1, dim=1).indices[0][0].item()\n","        adv_output = model(adv_example.unsqueeze(0).unsqueeze(0)).topk(1, dim=1).indices[0][0].item()\n","        ret.append((sample, label, target, adv_example, output, adv_output))\n","        counter += 1\n","\n","    torch.save(ret, f'adv_test_set_model{model_letter}_{start}_{end_exclusive}.pth')\n","    return ret\n","\n","    # return [(sample, label, target, get_optimized_x_adv(sample, model, target,\n","    # num_steps= 5, tau=0.05, kappa=0, lr=1, max_iter=1000, max_eval=None, tolerance_grad=1e-10, tolerance_change=1e-10, history_size=100, line_search_fn=\"strong_wolfe\")) for sample, label, target in testset_with_random_targets]\n","\n","\n","def save_statistics(list_of_tuples, name_of_file):\n","    # (0: sample, 1:label, 2:target, 3:adv_example, 4:output, 5:adv_output)\n","    correct_normal = sum(1 for z in list_of_tuples if z[1] == z[4])\n","    correct_targeted = sum(1 for z in list_of_tuples if z[2] == z[5])\n","    correct_untargeted = sum(1 for z in list_of_tuples if z[1] != z[5])\n","    print(f'performance accuracy: {correct_normal/len(list_of_tuples)}')\n","    print(f'attack success rate targeted: {correct_targeted/len(list_of_tuples)}')\n","    print(f'attack success rate untargeted: {correct_untargeted/len(list_of_tuples)}')\n","    torch.save([(correct_normal, correct_targeted, correct_untargeted)], f'{name_of_file}.pth')\n","\n","num_samples = 1000\n","### A\n","start_time = time.time()\n","adv_test_set_A = generate_adv_test_get(testset_with_random_targets, modelA, 0, num_samples, 'A')\n","save_statistics(adv_test_set_A, \"adv_test_set_A_results_0_10000\")\n","end_time = time.time()\n","duration = end_time - start_time\n","hours = duration // 3600\n","minutes = (duration % 3600) // 60\n","seconds = duration % 60\n","print(f\"Duration: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n","# ### B\n","# start_time = time.time()\n","# adv_test_set_A = generate_adv_test_get(testset_with_random_targets, modelB, 0, num_samples, 'B')\n","# save_statistics(adv_test_set_A, \"adv_test_set_B_results_0_10000\")\n","# end_time = time.time()\n","# duration = end_time - start_time\n","# hours = duration // 3600\n","# minutes = (duration % 3600) // 60\n","# seconds = duration % 60\n","# print(f\"Duration: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n","# ### C\n","# start_time = time.time()\n","# adv_test_set_A = generate_adv_test_get(testset_with_random_targets, modelC, 0, num_samples, 'C')\n","# save_statistics(adv_test_set_A, \"adv_test_set_C_results_0_10000\")\n","# end_time = time.time()\n","# duration = end_time - start_time\n","# hours = duration // 3600\n","# minutes = (duration % 3600) // 60\n","# seconds = duration % 60\n","# print(f\"Duration: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n","# ### Z = model from github\n","# start_time = time.time()\n","# adv_test_set_A = generate_adv_test_get(testset_with_random_targets, modelA, 0, num_samples, 'Z')\n","# save_statistics(adv_test_set_A, \"adv_test_set_Z_results_0_10000\")\n","# end_time = time.time()\n","# duration = end_time - start_time\n","# hours = duration // 3600\n","# minutes = (duration % 3600) // 60\n","# seconds = duration % 60\n","# print(f\"Duration: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n","\n","# adv_test_set_B = generate_adv_test_get(testset_with_random_targets, modelB, 0, 1000, 'B')\n","# adv_test_set_C = generate_adv_test_get(testset_with_random_targets, modelC, 0, 1000, 'C')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaOA_eV8f7lz"},"outputs":[],"source":["torch.save(adv_test_set_A,\"50_steps_1000_samples.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moBnX92of7l0"},"outputs":[],"source":["adv_test_set_A = torch.load(\"50_steps_1000_samples.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPgfiF9rf7l0"},"outputs":[],"source":["def plot_tensor(tensor):\n","    # Convert tensor to numpy array\n","    tensor_np = tensor.detach().cpu().numpy()  # Ensure it's detached from any computational graph and on CPU\n","\n","    # Plot the tensor\n","    plt.imshow(tensor_np, cmap='gray')  # Assuming grayscale image, adjust colormap as needed\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tr3wqacYf7l0","outputId":"1093796a-170f-4685-eb66-db3142b2501b"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJAUlEQVR4nO3cTajPaR/H8evHkbKhLCgLNQtRnhaOtQWyHooilCZ2MslTKUsLYik7GyzO0sqGDfKUhWxkgdKEheQYx9T0v1f3Z3PfU//vz3ly5vVan0+/yzRz3q7FXN1gMBg0AGitzZvpAwAwe4gCACEKAIQoABCiAECIAgAhCgCEKAAQI8P+YNd1U3kOAKbYMP+vspsCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAjM30A/r/Vq1f32u3YsaO8OXnyZHmzfPny8mYwGJQ3fT1+/Li8+f3338ube/fulTcwm7kpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAEQ3GPKVsq7rpvosc9aSJUvKm+fPn/f61rJly8qbhw8f9vpWVd8/0+joaHmzcePG8ubLly/lzfbt28ubJ0+elDcwGYb5de+mAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAexJulli9f3mu3Zs2a8ubOnTu9vjWbHTp0qLy5evVqefPp06fyZsOGDeVNa629e/eu1w7+y4N4AJSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAexGNOmj9/fnkzOjpa3ty/f7+8uX37dnnTWmu7du0qb8bHx3t9i7nJg3gAlIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEB7Egx9w4cKF8ubYsWO9vnXixIny5uLFi72+xdzkQTwASkQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAILySCj9g2bJl5c2zZ896fev79+/lzbp168qb8fHx8oafg1dSASgRBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBGZvoA8DN7//79tGxaa239+vXlzaZNm8qbu3fvljfMHW4KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOFBPJhmN27c6LXr8yDe6OhoeeNBvH83NwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA8CAeTLOXL1/O9BHgH7kpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECMzPQB4N9m1apVvXZd103ySeB/uSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhAfxYJqtXbu2124wGJQ3ExMTvb7Fv5ebAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB0gyFf2eq6bqrPwk9k48aN5c3KlSsn/yD/4MWLF+XNq1evypulS5eWN8+ePStvWmvt/v375c3+/fvLm7/++qu84ecwzK97NwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAGJnpAzDzLl26VN4cPny4vFm4cGF509eff/5Z3oyNjZU3r1+/Lm9WrFhR3rTW2vXr18sbj9tR5aYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQHgldY7ZunVreXPw4MHy5vHjx+XN+fPny5vWWlu8eHF5c+rUqfJm//795U3XdeXNYDAob2C6uCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoARDcY8nWuPg9/Mf0+fvxY3ixdurS8mTdvdv99YtGiReXNgwcPypv169eXN30fxHvz5k15c+7cufLm2rVr5Q0/h2H+3Zvd/2UDMK1EAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIiRmT4Ak6vPY2tjY2NTcJLJs3DhwvLm9OnT5c3atWvLm1evXpU3t27dKm9aa+3IkSPlzZUrV8qbX3/9tbzZvXt3eTMxMVHeMPXcFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQCiGwz5glrXdVN9FibBhw8fypuvX7+WN1u2bClvFixYUN601tqBAwfKmzNnzpQ3b968KW/6/HN4+/ZtedNaa5s3by5v9uzZU94cPXq0vLl582Z5s3fv3vKGHzPMr3s3BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAYmekDMLk+f/5c3vzyyy/lzfPnz8ubvo8q/v333+XNw4cPy5t9+/aVN30ft+vj0aNH5c3Tp0/Lmz4PJJ44caK8+eOPP8qb1lo7fvx4rx3DcVMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAILySOsds27atvPntt9/Km507d5Y33759K29aa+3kyZPlze3bt3t9a67p88Ls2bNny5t3796VN5cvXy5vWmttfHy8vDl//nx5MzExUd7MBW4KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCANENBoPBUD/YdVN9FgCm0DC/7t0UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACIkWF/cDAYTOU5AJgF3BQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACI/wA7hznv42MHTQAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["original number: 2\n","adversarial target: 0\n"]}],"source":["random_integer = random.randint(0, 999)\n","plot_tensor(adv_test_set_A[random_integer][3])\n","print(f\"original number: {adv_test_set_A[random_integer][1]}\")\n","print(f\"adversarial target: {adv_test_set_A[random_integer][2]}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
